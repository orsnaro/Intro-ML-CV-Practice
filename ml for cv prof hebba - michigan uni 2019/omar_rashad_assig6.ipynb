{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # **Name:** _Omar Rashad Salem_\n",
    "> # **Course:** _CV - prof.Heba_\n",
    "> # **Assignemnt No.:** _6_\n",
    "\n",
    "> ## QUESTIONS\n",
    "\n",
    "##### _1)_ **Explain the purpose of Batch Normalization in deep neural networks?**\n",
    "\n",
    "**ans:** \n",
    "Batch norm. layers help imporving model overall performance by specifically boosting training stability and accelerating training process.\n",
    "----\n",
    "\n",
    "##### _2)_ **Describe the two main steps in the Batch Normalization process: normalization and scale/shift?**\n",
    "\n",
    "**ans:**  \n",
    "- 1) normalization: trying to get each batch `mean` to `0` and `variance` to `1` by subtracting the mean from the batch and deviding by `STD`\n",
    "$$\n",
    "\\mu_j = \\frac{1}{N} \\sum_{i=1}^{N}x_{i,j}\n",
    "\\\\\n",
    "{\\sigma_j}^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_{i,j} - \\mu_j)^2\n",
    "\\\\\n",
    "\\therefore \\hat{x}_{i,j} = \\frac{x_{i,j} - \\mu_j}{\\sqrt{{\\sigma_j}^2 + \\epsilon}} (N \\times D)\n",
    "\n",
    "$$\n",
    "- 2) scale\\shift: introducing new 2 learnable hyperparameters `gamma` and `beta` that finds the optimal scale and shift value of the normalized batch which makes the model converge faster\n",
    "$$\n",
    "y = \\gamma \\hat{x}+ \\beta \n",
    "$$\n",
    "----\n",
    "##### _3)_ **How do the learnable parameters, gamma and beta, contribute to Batch Normalization?**\n",
    "\n",
    "**ans:** \n",
    "Gamma $\\gamma$ and beta $\\beta$ are parameters that are learned during training\n",
    "through backpropagation. These parameters enable the model to decide the optimal\n",
    "scale and shift for the normalized data.\n",
    "\n",
    "$$\n",
    "y_{i,j} = \\gamma_j \\hat{x}_{i,j}+ \\beta_j \n",
    "$$\n",
    "\n",
    "----\n",
    "##### _4)_ **What was the groundbreaking contribution of AlexNet to the field of Convolutional NeuralNetworks?**\n",
    "\n",
    "**ans:**\n",
    "- Introduced one of the first usefull Deep not shallow ConvNet models (Achieved best scores in 2012 ImageNet Classification Challenge error = `16.4%`)\n",
    "- Was one of the first models to use ReLU nonlinearities\n",
    "- Itroduced very early normilzation method called `local response normlization`\n",
    "\n",
    "----\n",
    "##### _5)_ **How does the VGG architecture differ from other ConvNet architectures in terms of filter size?**\n",
    "\n",
    "\n",
    "**ans:** \n",
    "Introduced a fixed rules to use:\n",
    "- only `3x3 s=1 p=1` conv filters making scalling the model much easier\n",
    "- only max pooling layers `2x2 s=2` filters\n",
    "\n",
    "----\n",
    "##### _6)_ **Explain the concept of the inception module in GoogLeNet and its advantages?**\n",
    "\n",
    "**ans:** \n",
    "- Inception module is a repeated local structure in GoogLeNet arch. works by simultaneously calculating multiple/all conv kernel sizes in same level then concatenate them  effectively  eleminating the need for kernel/filter size as a hyperparameter.\n",
    "\n",
    "- They also used bottleneck layer (filters) to reduce feature map dimensions before any expensive computation layers.\n",
    "\n",
    "----\n",
    "##### _7)_ **What is the main innovation introduced by ResNet to address training challenges in deepnetworks?**\n",
    "\n",
    "**ans:** \n",
    "- Before ResNets after specific deep/depth ConvNets  starts performing worse!\n",
    "- ResNets came to solve this issue by making learning the identity functions with high depth models easy and now deeper models again behaves as expeceted (better than shallower models)\n",
    "- After ResNet now we have two main blocks in Convnets a `Plain block` and a `Residual block`\n",
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Programming assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified VGG model Summary: \n",
      "\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 1, 1, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,930,314\n",
      "Trainable params: 15,930,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "modified ResNet model Summary: \n",
      "\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 1, 1, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,376,202\n",
      "Trainable params: 26,323,082\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Omar rashad note: un-comment next line if first time using keras \n",
    "#! pip install keras\n",
    "\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import Sequential\n",
    "from keras.applications import VGG16, ResNet50\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Flatten,Dense #MAY NEED: BatchNormalization,Activation,Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "#get the  cifar10\n",
    "from keras.datasets import cifar10\n",
    "(x_train,y_train),(x_test,y_test)=cifar10.load_data()\n",
    "\n",
    "#get validation subset\n",
    "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=.3)\n",
    "\n",
    "#turn to onehot\n",
    "y_train=to_categorical(y_train)\n",
    "y_val=to_categorical(y_val)\n",
    "y_test=to_categorical(y_test)\n",
    "\n",
    "#Defining the VGG and ResNet50\n",
    "vgg16_base_model = VGG16(include_top=False,weights='imagenet',input_shape=(32,32,3),classes=y_train.shape[1])\n",
    "resnet50_base_model = ResNet50(include_top=False,weights='imagenet',input_shape=(32,32,3),classes=y_train.shape[1])\n",
    "\n",
    "#create our own modified model ( append our dens layer at end)\n",
    "model= Sequential()\n",
    "model.add(vgg16_base_model) \n",
    "model.add(Flatten()) \n",
    "\n",
    "model2= Sequential()\n",
    "model2.add(resnet50_base_model) \n",
    "model2.add(Flatten()) \n",
    "\n",
    "#after creating ? yes add the layers!\n",
    "model.add(Dense(1024,activation=('relu'),input_dim=512))\n",
    "model.add(Dense(512,activation=('relu'))) \n",
    "model.add(Dense(256,activation=('relu'))) \n",
    "model.add(Dense(128,activation=('relu')))\n",
    "model.add(Dense(10,activation=('softmax'))) #This is the classification layer\n",
    "\n",
    "model2.add(Dense(1024,activation=('relu'),input_dim=512))\n",
    "model2.add(Dense(512,activation=('relu'))) \n",
    "model2.add(Dense(256,activation=('relu'))) \n",
    "model2.add(Dense(128,activation=('relu')))\n",
    "model2.add(Dense(10,activation=('softmax'))) #This is the classification layer\n",
    "\n",
    "#let's see it now! ORS#\n",
    "print(f'modified VGG model Summary: \\n')\n",
    "model.summary()\n",
    "print(f'modified ResNet model Summary: \\n')\n",
    "model2.summary()\n",
    "\n",
    "#hype hype :D hyperparameters \n",
    "batch_size= 100\n",
    "epochs=5\n",
    "learn_rate=.001\n",
    "sgd=SGD(learning_rate=learn_rate,momentum=.9,nesterov=False)\n",
    "\n",
    "#all set! COMPILE!\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model2.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#train and save train history\n",
    "print(f'\\n\\n\\n START TRAINING OUR ResNet \\n\\n\\n')\n",
    "history2 = model2.fit(x_train, y_train, batch_size= batch_size, epochs= epochs, validation_data=(x_val, y_val))\n",
    "print(f'\\n\\n\\n START TRAINING OUR VGG \\n\\n\\n')\n",
    "history = model.fit(x_train, y_train, batch_size= batch_size, epochs= epochs, validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Our VGG model train history: \n",
      "Epochs loss : [2.331294536590576, 2.302603244781494, 2.302593946456909, 2.3025898933410645, 2.3025882244110107]\n",
      "Epochs accuracy : [0.09759999811649323, 0.09868571162223816, 0.09757142513990402, 0.09991428256034851, 0.09797143191099167]\n",
      "Epochs val_loss : [2.3026533126831055, 2.30268931388855, 2.3027286529541016, 2.302746295928955, 2.3027658462524414]\n",
      "Epochs val_accuracy : [0.09880000352859497, 0.09753333032131195, 0.09753333032131195, 0.09746666997671127, 0.09746666997671127]\n",
      "\n",
      "\n",
      "Our ResNet model train history: \n",
      "Epochs loss : [1.592706561088562, 0.8918847441673279, 0.6395752429962158, 0.45397523045539856, 0.342894583940506]\n",
      "Epochs accuracy : [0.43308570981025696, 0.689542829990387, 0.7785428762435913, 0.8407999873161316, 0.8809428811073303]\n",
      "Epochs val_loss : [1.126675009727478, 0.8955151438713074, 0.856717050075531, 0.8438029289245605, 0.8935683369636536]\n",
      "Epochs val_accuracy : [0.6024666428565979, 0.6928666830062866, 0.7077333331108093, 0.73253333568573, 0.7376000285148621]\n",
      "\n",
      "\n",
      "\n",
      "Total Train Time for Both models = 88m 30.8s\n",
      "\n",
      "\n",
      "\n",
      "E V A L U A T E\n",
      "313/313 [==============================] - 18s 57ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Our VGG model final results on cifar10 dataset: \n",
      " loss= 2.3026132583618164\n",
      " accuracy= 0.10000000149011612\n",
      "\n",
      "\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.8945 - accuracy: 0.7362\n",
      "Our ResNet model final results on cifar10 dataset: \n",
      " loss= 0.894540548324585\n",
      " accuracy= 0.7361999750137329\n"
     ]
    }
   ],
   "source": [
    "#get the history\n",
    "print(f'\\n\\nOur VGG model train history: ')\n",
    "for info in history.history: print(f'Epochs {info} : {history.history[info]}')\n",
    "\n",
    "print(f'\\n\\nOur ResNet model train history: ')\n",
    "for info in history2.history: print(f'Epochs {info} : {history2.history[info]}')\n",
    "\n",
    "\n",
    "print('\\n\\n')\n",
    "print('Total Train Time for Both models = 88m 30.8s')\n",
    "print('\\n\\n')\n",
    "\n",
    "\n",
    "print('E V A L U A T E')\n",
    "#evaluate\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'Our VGG model final results on cifar10 dataset: \\n loss= {loss}\\n accuracy= {accuracy}\\n\\n')\n",
    "loss2, accuracy2 = model2.evaluate(x_test, y_test)\n",
    "print(f'Our ResNet model final results on cifar10 dataset: \\n loss= {loss2}\\n accuracy= {accuracy2}')\n",
    "\n",
    "#DONE!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
