{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # **Name:** _Omar Rashad Salem_\n",
    "> # **Course:** _CV - prof.Heba_\n",
    "> # **Assignemnt No.:** _4_\n",
    "\n",
    "> ## QUESTIONS\n",
    "\n",
    "##### _1)_ **What is a Convolutional Neural Network (CNN) and what makes it suitable for image-related tasks?**\n",
    "\n",
    "**ans:** \n",
    "- CNN  is type of neural networks that is used for processing  a higher dimensional type of data or cluster of data like images e.g.(rgb / gray scale) it posseses the ability to learn  patterns and hierarchical\n",
    " representations\n",
    "----\n",
    "\n",
    "##### _2)_ **Explain the concept of local connectivity in CNNs.**\n",
    "\n",
    "**ans:**  output of neuron in a X layer connects only to a small local region in the next layer that mostly share some similar context.  this effectively reduces `CNN` complexity and optimizes it!\n",
    "\n",
    "----\n",
    "##### _3)_ **What are the key parameters of a Convolutional Layer?**\n",
    "\n",
    "**ans:** \n",
    "- No. of filters `K`\n",
    "- Stride `S`\n",
    "- Filter size `F`\n",
    "- Zero padding `P`\n",
    "\n",
    "\n",
    "----\n",
    "##### _4)_ **Why is zero-padding used in Convolutional Layers, and what impact does it have on the spatial arrangement?**\n",
    "\n",
    "**ans:**\n",
    "- preserves the input dimensions / makes it managable\n",
    "- marks the edges of an images which could come very handy in image processing and computer vision\n",
    "\n",
    "----\n",
    "##### _5)_ **What is the purpose of pooling layer in ConvNet?**\n",
    "\n",
    "\n",
    "**ans:** \n",
    "- reduces the dimensions and parameters which helps boosting CNN over all performance and computing time\n",
    "- produces no extra parameters \n",
    "\n",
    "\n",
    "----\n",
    "##### _6)_ **SOLVE**\n",
    "given:\n",
    "\n",
    "\t\t- filter size = 2x2\n",
    " \t\t- stride = 2\n",
    "\t\t- i/p size = 28x28x64\n",
    "\t\t- it's a pooling layer\n",
    "\n",
    "* Find o/p Volume?\n",
    "\n",
    "**ans:** \n",
    "$$\n",
    "\tW = (w - f)/s + 1\n",
    "\t\\\\\n",
    "\tW = (16 - 2) / 2 + 1\n",
    "\t\\\\\n",
    "\t\\therefore W = 14\n",
    "\t\\\\\n",
    "\to/p-Volume = 14 * 14 * 64\n",
    "\t\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "##### _7)_ **SOLVE**\n",
    "\n",
    "given:\n",
    "\n",
    "\t\t- filter size = 3x3\n",
    "\t\t- filters No. = 32\n",
    " \t\t- stride = 1\n",
    "\t\t- i/p size = 32x32x3\n",
    "\t\t- it's a Convolution layer\n",
    "\n",
    "* Find o/p Volume?\n",
    "\n",
    "**ans:** \n",
    "$$\n",
    "\tW = (w - f + 2p)/s + 1\n",
    "\t\\\\\n",
    "\tW = (32 - 3 + 2*0) / 1 + 1\n",
    "\t\\\\\n",
    "\t\\therefore W = 30\n",
    "\t\\\\\n",
    "\to/p-Volume = 30 * 30 * 32\n",
    "\t\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "##### _8)_ **SOLVE**\n",
    "\n",
    "given:\n",
    "\n",
    "\t\t- filter size = 3x3\n",
    "\t\t- filters No. = 8\n",
    " \t\t\n",
    "* Find No. of parameters including biases?\n",
    "\n",
    "**ans:** \n",
    "$$\n",
    "\tNo.parameters = filter'size * depth * biases\n",
    "\t\\\\\n",
    "\t= 3 * 3 * 3 + 1 \n",
    "\t\\\\\n",
    "\to/p-Volume = 28 \n",
    "\t\\\\\n",
    "\t\\therefore No.parameters = 28*8 = 224\n",
    "$$\n",
    "\n",
    "\n",
    "----\n",
    "##### _9)_ **SOLVE**\n",
    "\n",
    "given:\n",
    "\n",
    "\t\t- i/p volume = 256*256*3\n",
    "\t\t- layer neurons no. = 100\n",
    " \t\t\n",
    "* Find No. weights and biases needed?\n",
    "\n",
    "**ans:** \n",
    "$$\n",
    "\tweights = 256*256*3*100 \n",
    "\t\\\\\n",
    "\tweights = 19660800\n",
    "\t\\\\\n",
    "\tbiases = 1*100 = 100\n",
    "$$\n",
    "\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Programming Assignment _(challenge task!)_: \n",
    "> #### Image Classification using Convolutional Neural Networks (ConvNet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Code for accuracy FOCUSED CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imgaug in c:\\python311\\lib\\site-packages (0.4.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: six in c:\\python311\\lib\\site-packages (from imgaug) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\python311\\lib\\site-packages (from imgaug) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\python311\\lib\\site-packages (from imgaug) (1.9.3)\n",
      "Requirement already satisfied: Pillow in c:\\python311\\lib\\site-packages (from imgaug) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\python311\\lib\\site-packages (from imgaug) (3.6.2)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in c:\\python311\\lib\\site-packages (from imgaug) (0.20.0)\n",
      "Requirement already satisfied: opencv-python in c:\\python311\\lib\\site-packages (from imgaug) (4.7.0.72)\n",
      "Requirement already satisfied: imageio in c:\\python311\\lib\\site-packages (from imgaug) (2.28.0)\n",
      "Requirement already satisfied: Shapely in c:\\python311\\lib\\site-packages (from imgaug) (2.0.2)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\python311\\lib\\site-packages (from scikit-image>=0.14.2->imgaug) (3.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\python311\\lib\\site-packages (from scikit-image>=0.14.2->imgaug) (2023.4.12)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\python311\\lib\\site-packages (from scikit-image>=0.14.2->imgaug) (1.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\omarpc\\appdata\\roaming\\python\\python311\\site-packages (from scikit-image>=0.14.2->imgaug) (21.3)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\python311\\lib\\site-packages (from scikit-image>=0.14.2->imgaug) (0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib->imgaug) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python311\\lib\\site-packages (from matplotlib->imgaug) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python311\\lib\\site-packages (from matplotlib->imgaug) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib->imgaug) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\omarpc\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->imgaug) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\omarpc\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->imgaug) (2.8.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\n",
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 65536)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8388736   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,427,914\n",
      "Trainable params: 8,427,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model info: \n",
      " None\n",
      "Epoch 1/100\n",
      "40/40 [==============================] - 54s 1s/step - loss: 19.8567 - accuracy: 0.1136 - val_loss: 2.2892 - val_accuracy: 0.1752\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 52s 1s/step - loss: 2.0838 - accuracy: 0.2416 - val_loss: 2.1307 - val_accuracy: 0.2400\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 57s 1s/step - loss: 1.8203 - accuracy: 0.3464 - val_loss: 2.7642 - val_accuracy: 0.1569\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 54s 1s/step - loss: 1.3308 - accuracy: 0.5512 - val_loss: 2.2351 - val_accuracy: 0.2976\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 56s 1s/step - loss: 0.7881 - accuracy: 0.7296 - val_loss: 2.8323 - val_accuracy: 0.2866\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 52s 1s/step - loss: 0.4342 - accuracy: 0.8544 - val_loss: 3.1078 - val_accuracy: 0.2897\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 51s 1s/step - loss: 0.1905 - accuracy: 0.9384 - val_loss: 3.8958 - val_accuracy: 0.2930\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 51s 1s/step - loss: 0.1328 - accuracy: 0.9640 - val_loss: 4.1778 - val_accuracy: 0.2896\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 54s 1s/step - loss: 0.0566 - accuracy: 0.9832 - val_loss: 4.8738 - val_accuracy: 0.3055\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 53s 1s/step - loss: 0.0620 - accuracy: 0.9816 - val_loss: 4.1040 - val_accuracy: 0.2993\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 51s 1s/step - loss: 0.0461 - accuracy: 0.9880 - val_loss: 4.6708 - val_accuracy: 0.3036\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 49s 1s/step - loss: 0.0246 - accuracy: 0.9952 - val_loss: 5.2148 - val_accuracy: 0.2924\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 50s 1s/step - loss: 0.0240 - accuracy: 0.9944 - val_loss: 5.7299 - val_accuracy: 0.3039\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 50s 1s/step - loss: 0.0118 - accuracy: 0.9960 - val_loss: 5.8724 - val_accuracy: 0.2935\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 52s 1s/step - loss: 0.0295 - accuracy: 0.9912 - val_loss: 6.4842 - val_accuracy: 0.2914\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 51s 1s/step - loss: 0.0229 - accuracy: 0.9936 - val_loss: 5.1739 - val_accuracy: 0.2897\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 50s 1s/step - loss: 0.1482 - accuracy: 0.9560 - val_loss: 5.3341 - val_accuracy: 0.2732\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 52s 1s/step - loss: 0.0632 - accuracy: 0.9784 - val_loss: 5.4907 - val_accuracy: 0.2939\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 50s 1s/step - loss: 0.0355 - accuracy: 0.9936 - val_loss: 5.4527 - val_accuracy: 0.2857\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 51s 1s/step - loss: 0.0227 - accuracy: 0.9936 - val_loss: 5.2418 - val_accuracy: 0.2958\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 53s 1s/step - loss: 0.0119 - accuracy: 0.9984 - val_loss: 6.6334 - val_accuracy: 0.2902\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 57s 1s/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 6.5878 - val_accuracy: 0.3005\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 55s 1s/step - loss: 7.0884e-04 - accuracy: 1.0000 - val_loss: 6.9459 - val_accuracy: 0.3006\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 54s 1s/step - loss: 2.7547e-04 - accuracy: 1.0000 - val_loss: 7.1389 - val_accuracy: 0.3014\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 54s 1s/step - loss: 2.1157e-04 - accuracy: 1.0000 - val_loss: 7.2960 - val_accuracy: 0.3012\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 52s 1s/step - loss: 1.6794e-04 - accuracy: 1.0000 - val_loss: 7.4253 - val_accuracy: 0.3009\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 52s 1s/step - loss: 1.4050e-04 - accuracy: 1.0000 - val_loss: 7.5268 - val_accuracy: 0.3011\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 52s 1s/step - loss: 1.1853e-04 - accuracy: 1.0000 - val_loss: 7.6200 - val_accuracy: 0.3010\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 52s 1s/step - loss: 1.0385e-04 - accuracy: 1.0000 - val_loss: 7.7070 - val_accuracy: 0.3008\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 51s 1s/step - loss: 8.9937e-05 - accuracy: 1.0000 - val_loss: 7.7629 - val_accuracy: 0.3003\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 51s 1s/step - loss: 8.1216e-05 - accuracy: 1.0000 - val_loss: 7.8412 - val_accuracy: 0.3003\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 57s 1s/step - loss: 7.2502e-05 - accuracy: 1.0000 - val_loss: 7.9046 - val_accuracy: 0.3003\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 57s 1s/step - loss: 6.6209e-05 - accuracy: 1.0000 - val_loss: 7.9597 - val_accuracy: 0.3005\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 56s 1s/step - loss: 5.9631e-05 - accuracy: 1.0000 - val_loss: 8.0149 - val_accuracy: 0.3006\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 55s 1s/step - loss: 5.4898e-05 - accuracy: 1.0000 - val_loss: 8.0590 - val_accuracy: 0.3007\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 56s 1s/step - loss: 5.0848e-05 - accuracy: 1.0000 - val_loss: 8.0971 - val_accuracy: 0.3008\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 55s 1s/step - loss: 4.6731e-05 - accuracy: 1.0000 - val_loss: 8.1464 - val_accuracy: 0.3004\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 54s 1s/step - loss: 4.3575e-05 - accuracy: 1.0000 - val_loss: 8.1906 - val_accuracy: 0.3004\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 48s 1s/step - loss: 4.0668e-05 - accuracy: 1.0000 - val_loss: 8.2259 - val_accuracy: 0.3003\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 48s 1s/step - loss: 3.7709e-05 - accuracy: 1.0000 - val_loss: 8.2655 - val_accuracy: 0.3002\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 50s 1s/step - loss: 3.5255e-05 - accuracy: 1.0000 - val_loss: 8.3041 - val_accuracy: 0.3001\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 55s 1s/step - loss: 3.3193e-05 - accuracy: 1.0000 - val_loss: 8.3495 - val_accuracy: 0.3001\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 57s 1s/step - loss: 3.1072e-05 - accuracy: 1.0000 - val_loss: 8.3810 - val_accuracy: 0.3000\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 56s 1s/step - loss: 2.9320e-05 - accuracy: 1.0000 - val_loss: 8.4142 - val_accuracy: 0.2998\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 54s 1s/step - loss: 2.7721e-05 - accuracy: 1.0000 - val_loss: 8.4490 - val_accuracy: 0.2998\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 53s 1s/step - loss: 2.6224e-05 - accuracy: 1.0000 - val_loss: 8.4764 - val_accuracy: 0.2996\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 56s 1s/step - loss: 2.4756e-05 - accuracy: 1.0000 - val_loss: 8.5038 - val_accuracy: 0.2994\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 58s 1s/step - loss: 2.3482e-05 - accuracy: 1.0000 - val_loss: 8.5462 - val_accuracy: 0.2992\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 58s 1s/step - loss: 2.5516e-05 - accuracy: 1.0000 - val_loss: 8.7062 - val_accuracy: 0.2989\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 59s 2s/step - loss: 2.1053e-05 - accuracy: 1.0000 - val_loss: 8.7077 - val_accuracy: 0.2993\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 56s 1s/step - loss: 1.9144e-05 - accuracy: 1.0000 - val_loss: 8.7155 - val_accuracy: 0.2994\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 52s 1s/step - loss: 1.7697e-05 - accuracy: 1.0000 - val_loss: 8.7334 - val_accuracy: 0.2997\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 53s 1s/step - loss: 1.6664e-05 - accuracy: 1.0000 - val_loss: 8.7535 - val_accuracy: 0.2999\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 51s 1s/step - loss: 1.5835e-05 - accuracy: 1.0000 - val_loss: 8.7747 - val_accuracy: 0.2997\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 49s 1s/step - loss: 1.4981e-05 - accuracy: 1.0000 - val_loss: 8.7941 - val_accuracy: 0.2996\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 49s 1s/step - loss: 1.4323e-05 - accuracy: 1.0000 - val_loss: 8.8142 - val_accuracy: 0.2999\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 49s 1s/step - loss: 1.3665e-05 - accuracy: 1.0000 - val_loss: 8.8388 - val_accuracy: 0.2997\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 48s 1s/step - loss: 1.3113e-05 - accuracy: 1.0000 - val_loss: 8.8571 - val_accuracy: 0.2998\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 43s 1s/step - loss: 1.2499e-05 - accuracy: 1.0000 - val_loss: 8.8875 - val_accuracy: 0.2995\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 47s 1s/step - loss: 1.2003e-05 - accuracy: 1.0000 - val_loss: 8.9098 - val_accuracy: 0.2996\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 50s 1s/step - loss: 1.1537e-05 - accuracy: 1.0000 - val_loss: 8.9294 - val_accuracy: 0.2996\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 48s 1s/step - loss: 1.1072e-05 - accuracy: 1.0000 - val_loss: 8.9519 - val_accuracy: 0.2996\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 47s 1s/step - loss: 1.0658e-05 - accuracy: 1.0000 - val_loss: 8.9723 - val_accuracy: 0.2996\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 48s 1s/step - loss: 1.0247e-05 - accuracy: 1.0000 - val_loss: 8.9954 - val_accuracy: 0.2994\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 50s 1s/step - loss: 9.8632e-06 - accuracy: 1.0000 - val_loss: 9.0138 - val_accuracy: 0.2994\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 52s 1s/step - loss: 9.5093e-06 - accuracy: 1.0000 - val_loss: 9.0353 - val_accuracy: 0.2995\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 53s 1s/step - loss: 9.1489e-06 - accuracy: 1.0000 - val_loss: 9.0629 - val_accuracy: 0.2995\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - ETA: 0s - loss: 8.8436e-06 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\omar-work\\tasks college\\img process prof hebba\\omar_rashad_assig5_P1_v2.ipynb Cell 4\u001b[0m in \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/omar-work/tasks%20college/img%20process%20prof%20hebba/omar_rashad_assig5_P1_v2.ipynb#W3sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m#run model and get the results\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/omar-work/tasks%20college/img%20process%20prof%20hebba/omar_rashad_assig5_P1_v2.ipynb#W3sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/omar-work/tasks%20college/img%20process%20prof%20hebba/omar_rashad_assig5_P1_v2.ipynb#W3sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(xtr, ytr, epochs\u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(xeval, yeval))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/omar-work/tasks%20college/img%20process%20prof%20hebba/omar_rashad_assig5_P1_v2.ipynb#W3sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m tst_loss, tst_accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(xtst, ytst)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/omar-work/tasks%20college/img%20process%20prof%20hebba/omar_rashad_assig5_P1_v2.ipynb#W3sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m#print result and save model weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\engine\\training.py:1729\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1714\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1715\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[0;32m   1716\u001b[0m         x\u001b[39m=\u001b[39mval_x,\n\u001b[0;32m   1717\u001b[0m         y\u001b[39m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1727\u001b[0m         steps_per_execution\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution,\n\u001b[0;32m   1728\u001b[0m     )\n\u001b[1;32m-> 1729\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m   1730\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[0;32m   1731\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[0;32m   1732\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[0;32m   1733\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[0;32m   1734\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   1735\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1736\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1737\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1738\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1739\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1740\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1741\u001b[0m )\n\u001b[0;32m   1742\u001b[0m val_logs \u001b[39m=\u001b[39m {\n\u001b[0;32m   1743\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   1744\u001b[0m }\n\u001b[0;32m   1745\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\engine\\training.py:2072\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2068\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   2069\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m   2070\u001b[0m ):\n\u001b[0;32m   2071\u001b[0m     callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 2072\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_function(iterator)\n\u001b[0;32m   2073\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   2074\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    931\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    934\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    935\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%pip install imgaug\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imgaug import augmenters as iaa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization, GlobalMaxPooling2D\n",
    "\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "(xtr, ytr), (xtst, ytst) = cifar10.load_data()\n",
    "xeval, xtr, yeval, ytr = train_test_split(  xtr, ytr, test_size= 1000, random_state= 42 )\n",
    "  \n",
    "\n",
    "# augment 25% of data and append it to train set (reform some of image to widen the range of cases that model will see)\n",
    "num_subset = 0.25 * ytr.shape[0]\n",
    "\n",
    "rand_subset_indices = np.random.choice(ytr.shape[0], int(num_subset), replace=False)\n",
    "image_subset = xtr[rand_subset_indices]\n",
    "lables_subset = ytr[rand_subset_indices]\n",
    "\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "   iaa.Fliplr(0.5), #horizental\n",
    "   iaa.Crop(percent=(0, 0.1)),\n",
    "   iaa.Sometimes(\n",
    "      0.5,\n",
    "      iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "   ),\n",
    "   iaa.LinearContrast((0.75, 1.5)),\n",
    "   iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "   iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "   iaa.Affine(\n",
    "      scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "      shear=(-8, 8)\n",
    "   )], random_order= True)\n",
    "\n",
    "augmented_subset = seq(images= image_subset)\n",
    "\n",
    "#get the old images with new augmanted images (50k + 12.5k images)\n",
    "xtr = np.concatenate((xtr, augmented_subset), axis= 0)\n",
    "ytr = np.concatenate((ytr, lables_subset), axis= 0)\n",
    "\n",
    "\n",
    "# Build Accuracy focused CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation= 'relu', padding= 'same', input_shape= (32, 32, 3)))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), activation= 'relu', padding='same'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), activation= 'relu', padding='same'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), activation= 'relu', padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(GlobalMaxPooling2D()) #only focus on most important features (not mainly used to reduce dimensions like maxpooling2d )\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation= 'relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation= 'softmax'))\n",
    "\n",
    "print (f\"Model info: \\n {model.summary()}\")\n",
    "\n",
    "#run model and get the results\n",
    "model.compile(optimizer= 'adam', loss= 'sparse_categorical_crossentropy', metrics= ['accuracy'])\n",
    "model.fit(xtr, ytr, epochs= 100, validation_data=(xeval, yeval))\n",
    "tst_loss, tst_accuracy = model.evaluate(xtst, ytst)\n",
    "\n",
    "#print result and save model weights\n",
    "print(f\"Model Performance Result(loss,accuracy): \\n {tst_loss,tst_accuracy}\")\n",
    "model.save(r\"./assig5_model_data/assig5_modelV3.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
