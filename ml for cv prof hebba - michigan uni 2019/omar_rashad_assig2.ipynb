{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Name:_ Omar Rashad Salem\n",
    "# _Course:_ CV - prof.Heba\n",
    "# _Assignemnt No.:_ 2\n",
    "\n",
    "> ## QUESTIONS\n",
    "\n",
    "#### _1)_ **What happens to the softmax loss when the predicted probability for the true label increases?**\n",
    "**ans**: Softmax loss decreases ( SVM can get to 0 loss alot  easier but  Softmax continues to get better _(decrease loss)_ with better true lable scores).\n",
    "\n",
    "----\n",
    "\n",
    "#### _2)_  **What is the key difference between softmax loss and multi-class SVM loss in terms of their approach to handling multi-class classification?**\n",
    "**ans**: multiclass SVM is margin based loss function while Softmax is has probabilistic & log-likelihood approach.\n",
    "\n",
    "----\n",
    "\n",
    "#### _3)_  **For the multiclass SVM loss**\n",
    "$$\n",
    "Li = \\sum_{j \\neq y_i}{\\max(0, s_j - S_y, +1)}\n",
    "$$\n",
    "_a._  **At initialization W is small so all s â‰ˆ 0. What is the loss Li, assuming N examples and C classes?**\n",
    "\n",
    "**3a-ans**: $ (C-1) $\n",
    "\n",
    "_b._   **What if the sum was over all classes? _(Including ð‘—  =  ð‘¦ð‘– )_**\n",
    "\n",
    "**3b-ans**: the final loss will be increased by $1*N$ \n",
    "\n",
    "so for $ s â‰ˆ 0 $ answer = $ [(C-1) + 1] $\n",
    "\n",
    "----\n",
    "\n",
    "#### _4)_  **For the following scores calculate softmax and multiclass SVM loss if the first class is the correct class.**\n",
    "a. [10, -100, -100] \n",
    "\n",
    "b. [10, 9, 9]\n",
    "\n",
    "**4a-ans**: \n",
    "\n",
    "$$\n",
    "\\text{for SVM loss:}\n",
    "\\\\\n",
    "max(0, -100 + 10 + 1) = 0\n",
    "\\\\\n",
    "max(0, -100 + 10 + 1) = 0\n",
    "\\\\\n",
    "L_i = 0 + 0\n",
    "\\\\\n",
    "\\therefore L(SVM) = 0 \n",
    "\\\\\n",
    ".\n",
    "\\\\\n",
    "\\text{for Softmax loss:}\n",
    "\\\\\n",
    "\\text{take exp. of all scores: }  [e^{10} , e^{-100} , e^{-100}] = [22026.47 , 3.72*10^{-44}, 3.72*10^{-44}]\n",
    "\\\\\n",
    "\\text{normalize: } [1 , ~0 , ~0]\n",
    "\\\\\n",
    "-ln(1) > 0    \\text{ positive small value}\n",
    "\\\\\n",
    "\\therefore L_i(Softmax) â‰ˆ 0\n",
    "$$\n",
    "**4b-ans**: \n",
    "\n",
    "$$\n",
    "\\text{for SVM loss:}\n",
    "\\\\\n",
    "max(0, 9 - 10 + 1) = 0\n",
    "\\\\\n",
    "max(0, 9 - 10 + 1) = 0\n",
    "\\\\\n",
    "L_i = 0 + 0\n",
    "\\\\\n",
    "\\therefore L_i(SVM)  0 \n",
    "\\\\\n",
    ".\n",
    "\\\\\n",
    "\\text{for Softmax loss:}\n",
    "\\\\\n",
    "\\text{take exp. of all scores: }  [e^{10} , e^{9} , e^{9}] = [22026.47 , 8103.1 , 8103.1]\n",
    "\\\\\n",
    "\\text{normalize: } [0.5761 , 0.2119 , 0.2119]\n",
    "\\\\\n",
    "-ln(0.5761) = 0.5515\n",
    "\\\\\n",
    "\\therefore L_i(Softmax) = 0.5515\n",
    "$$\n",
    "\n",
    "##### **What does the calculated loss tells you about the difference between the two losses?**\n",
    "\n",
    "**ans:** the **SVM** can easily equal **0** loss whenever it's satisfied (which only needs the true label to be  higher by 1 from any of other scores) \n",
    "also changing the scores a little bit for **SVM** probably won't change the **0** loss result. **Softmax** on other hand never fully satisfied but it gets better and better values and separates the true label from other labels more and more by increasing it's probability value\n",
    "\n",
    "----\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## QUESTION 5 ( programming assignment )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing 'calc_loss' fucntion \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_loss( img : np.ndarray, W: np.ndarray, true_label: int): #NOTE: assume 3 classes 1)cat, 2)dog, 3)ship\n",
    "   classes = [\"cat\", \"dog\", \"ship\"] #'true_label' arg is the true label index\n",
    "   \n",
    "   #NOTE: first do linear classifier f(x,W) = Wx + biase\n",
    "   flattened_img = img.flatten()\n",
    "   pixels = len(flattened_img)\n",
    "   \n",
    "   #biase vector \n",
    "   biase = np.ndarray( shape= (len(classes),1) )\n",
    "   biase.fill(1)\n",
    "   \n",
    "   #cross product Wx\n",
    "   flattened_img = flattened_img.reshape(pixels,1)\n",
    "   Wx = np.dot(W , flattened_img) \n",
    "   \n",
    "   linear_classifier_scores = np.add(Wx , biase) #shape = (classes no,)\n",
    "   linear_classifier_scores = np.array([3.2,5.1,-1.7])\n",
    "   \n",
    "   \n",
    "   #NOTE: second do multi class SVM loss\n",
    "   true_lablel_score = linear_classifier_scores [int(true_label)]\n",
    "   \n",
    "   #max(0 , sj - Syi + 1)\n",
    "   maximums = [max(0 , score - true_lablel_score + 1) for score in linear_classifier_scores ] \n",
    "   SVM_Loss = np.sum(maximums) - 1\n",
    "   \n",
    "   \n",
    "   #NOTE: finally  do Softmax loss \n",
    "   \n",
    "   #get the exp value of all scores\n",
    "   exp_vec = np.exp(linear_classifier_scores )\n",
    "   \n",
    "   #get the normalized value of all scores\n",
    "   non_normalized_sum = np.sum(exp_vec)\n",
    "   normalized_value = exp_vec[true_label] / non_normalized_sum #I need to know only true label val\n",
    "   true_label_probability = normalized_value\n",
    "   \n",
    "   #get the Softmax loss\n",
    "   SOFTMAX_Loss = -np.log(true_label_probability)  #natural log ln()\n",
    "   \n",
    "   \n",
    "   return SOFTMAX_Loss , SVM_Loss\n",
    "   \n",
    "   # 24.5325  164.0219  0.1826835  188.73708\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax loss = 2.04035515280017 \n",
      "SVM loss = 2.8999999999999995\n"
     ]
    }
   ],
   "source": [
    "#using the 'calc_loss' function\n",
    "img = np.array([[56,231],\n",
    "                [24,2]])\n",
    "w =np.array([[0.2,-0.5,0.1,2.0],\n",
    "             [1.5,1.3,2.1,0.0],\n",
    "             [0,0.25,0.2,-0.3]])\n",
    "\n",
    "softmax_loss , svm_loss = calc_loss(img= img, W= w , true_label= 0)\n",
    "\n",
    "print (f\"softmax loss = {softmax_loss} \\nSVM loss = {svm_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
